#!/usr/bin/env python3
"""
Diagnostic tool to check pipeline health and data flow
"""
import os
import json
from pathlib import Path
from datetime import datetime

def print_header(title):
    print("\n" + "=" * 70)
    print(f"  {title}")
    print("=" * 70)

def check_directories():
    """Check if required directories exist and show contents"""
    print_header("üìÅ DIRECTORY CHECK")
    
    dirs = {
        "Bronze Layer": "/opt/airflow/data/bronze",
        "Gold Layer": "/opt/airflow/data/gold",
        "Temp Directory": "/tmp/tfl_data",
        "Scripts": "/opt/airflow/scripts",
        "DAGs": "/opt/airflow/dags",
    }
    
    for name, path in dirs.items():
        p = Path(path)
        if p.exists():
            files = list(p.iterdir()) if p.is_dir() else []
            print(f"‚úÖ {name:20s} EXISTS  ({len(files)} items)")
            
            # Show details for data directories
            if "data" in path or "tmp" in path:
                for f in files[:5]:  # Show first 5 items
                    if f.is_file():
                        size = f.stat().st_size
                        print(f"   ‚îî‚îÄ {f.name} ({size:,} bytes)")
        else:
            print(f"‚ùå {name:20s} MISSING")

def check_bronze_data():
    """Check bronze layer data quality"""
    print_header("üîç BRONZE LAYER DATA CHECK")
    
    bronze_dir = Path("/opt/airflow/data/bronze")
    
    if not bronze_dir.exists():
        print("‚ùå Bronze directory does not exist!")
        return
    
    json_files = list(bronze_dir.glob("*.json"))
    
    if not json_files:
        print("‚ö†Ô∏è  No JSON files found in bronze layer")
        print("üí° Run the ingestion task first: ingest_tfl_data -> consume_kafka_to_bronze")
        return
    
    print(f"üìä Found {len(json_files)} JSON file(s)")
    
    # Analyze first file
    for json_file in json_files[:3]:
        print(f"\nüìÑ Analyzing: {json_file.name}")
        
        try:
            with open(json_file, 'r') as f:
                data = json.load(f)
                
            # Determine if it's a list or dict
            if isinstance(data, list):
                print(f"   Type: List with {len(data)} items")
                if data:
                    sample = data[0]
                    print(f"   Sample keys: {list(sample.keys())[:10]}")
            elif isinstance(data, dict):
                print(f"   Type: Dictionary with {len(data)} keys")
                print(f"   Keys: {list(data.keys())[:10]}")
            else:
                print(f"   Type: {type(data)}")
                
        except json.JSONDecodeError as e:
            print(f"   ‚ùå Invalid JSON: {e}")
        except Exception as e:
            print(f"   ‚ùå Error reading file: {e}")

def check_gold_data():
    """Check gold layer data"""
    print_header("üíé GOLD LAYER DATA CHECK")
    
    gold_dir = Path("/opt/airflow/data/gold")
    
    if not gold_dir.exists():
        print("‚ö†Ô∏è  Gold directory does not exist")
        return
    
    parquet_files = list(gold_dir.glob("*.parquet"))
    
    if not parquet_files:
        print("‚ö†Ô∏è  No parquet files found in gold layer")
        print("üí° Run the transform task: transform_with_spark")
        return
    
    print(f"‚úÖ Found {len(parquet_files)} parquet file(s)")
    
    total_size = sum(f.stat().st_size for f in parquet_files)
    print(f"üìä Total size: {total_size:,} bytes ({total_size/1024/1024:.2f} MB)")

def check_kafka_connectivity():
    """Test Kafka connection"""
    print_header("üîå KAFKA CONNECTIVITY CHECK")
    
    try:
        from kafka import KafkaConsumer
        from kafka.errors import NoBrokersAvailable
        
        consumer = KafkaConsumer(
            bootstrap_servers='kafka:9092',
            consumer_timeout_ms=5000
        )
        
        topics = consumer.topics()
        print(f"‚úÖ Successfully connected to Kafka")
        print(f"üìã Available topics: {list(topics)}")
        
        consumer.close()
        
    except NoBrokersAvailable:
        print("‚ùå Cannot connect to Kafka broker")
        print("üí° Check if Kafka service is running: docker-compose ps")
    except ImportError:
        print("‚ö†Ô∏è  kafka-python not installed")
    except Exception as e:
        print(f"‚ùå Kafka check failed: {e}")

def check_pyspark():
    """Test PySpark installation"""
    print_header("‚ö° PYSPARK CHECK")
    
    try:
        from pyspark.sql import SparkSession
        
        # Try to create a minimal Spark session
        spark = SparkSession.builder \
            .appName("diagnostic_test") \
            .master("local[1]") \
            .config("spark.driver.memory", "512m") \
            .getOrCreate()
        
        # Create a simple DataFrame
        data = [("test", 1)]
        df = spark.createDataFrame(data, ["col1", "col2"])
        count = df.count()
        
        spark.stop()
        
        print(f"‚úÖ PySpark is working correctly")
        print(f"   Test DataFrame created with {count} row(s)")
        
    except ImportError as e:
        print(f"‚ùå PySpark not installed: {e}")
    except Exception as e:
        print(f"‚ùå PySpark test failed: {e}")

def check_aws_credentials():
    """Check AWS configuration"""
    print_header("‚òÅÔ∏è  AWS CONFIGURATION CHECK")
    
    aws_key = os.environ.get('AWS_ACCESS_KEY_ID', None)
    aws_secret = os.environ.get('AWS_SECRET_ACCESS_KEY', None)
    aws_region = os.environ.get('AWS_DEFAULT_REGION', None)
    s3_bucket = os.environ.get('S3_BUCKET', None)
    
    if aws_key:
        print(f"‚úÖ AWS_ACCESS_KEY_ID: {aws_key[:8]}...")
    else:
        print("‚ùå AWS_ACCESS_KEY_ID not set")
        
    if aws_secret:
        print(f"‚úÖ AWS_SECRET_ACCESS_KEY: {'*' * 20}")
    else:
        print("‚ùå AWS_SECRET_ACCESS_KEY not set")
        
    if aws_region:
        print(f"‚úÖ AWS_DEFAULT_REGION: {aws_region}")
    else:
        print("‚ö†Ô∏è  AWS_DEFAULT_REGION not set (will use default)")
        
    if s3_bucket:
        print(f"‚úÖ S3_BUCKET: {s3_bucket}")
    else:
        print("‚ö†Ô∏è  S3_BUCKET not set")
    
    # Try to connect to S3
    if aws_key and aws_secret:
        try:
            import boto3
            s3 = boto3.client('s3')
            buckets = s3.list_buckets()
            print(f"\n‚úÖ Successfully connected to AWS S3")
            print(f"üì¶ Available buckets: {len(buckets.get('Buckets', []))}")
        except Exception as e:
            print(f"\n‚ùå Failed to connect to AWS: {e}")

def main():
    """Run all diagnostic checks"""
    print("\n")
    print("üè•" * 35)
    print("  TfL PIPELINE HEALTH CHECK")
    print("  " + datetime.now().strftime("%Y-%m-%d %H:%M:%S UTC"))
    print("üè•" * 35)
    
    check_directories()
    check_bronze_data()
    check_gold_data()
    check_kafka_connectivity()
    check_pyspark()
    check_aws_credentials()
    
    print("\n" + "=" * 70)
    print("  DIAGNOSTIC CHECK COMPLETE")
    print("=" * 70 + "\n")

if __name__ == "__main__":
    main()